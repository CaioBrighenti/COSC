\documentclass{article}
\usepackage{amsmath} %This allows me to use the align functionality.
                     %If you find yourself trying to replicate
                     %something you found online, ensure you're
                     %loading the necessary packages!
\usepackage{amsfonts}%Math font
\usepackage{graphicx}%For including graphics
\usepackage{hyperref}%For Hyperlinks
\usepackage{listings}
\usepackage{graphicx}
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography
\usepackage[margin=1.0in]{geometry}
\usepackage{float}
\begin{document}
%set the size of the graphs to fit nicely on a 8.5x11 sheet
\noindent \textbf{Caio Brighenti }\\
\noindent \textbf{COSC 480 - Learning From Data}\\%\\ gives you a new line
\noindent \textbf{Fall 2019}\\%\\ gives you a new line
\noindent \textbf{Problem Set 5}\vspace{1em}\\
\begin{enumerate}
	\item Let $X_1$ and $X_2$ be independent random variables. Let $Y = X_1 + X_2$. Show that $V(Y) = V(X_1) + V(X_2)$.
	\\\\ Before showing $V(Y) = V(X_1) + V(X_2)$, we first start by showing a property that will be helpful in this proof. Let $Z = E[cX_1X_2]$ where $X_1,X_2$ are independent random variables and $c$ is a constant. We would like to show that the constant can be pulled out such that $Z = cE[X_1X_2]$. 
	\\\\ Let $C$ be a random variable with a single possible outcome: $c$. The probability that $C$ takes on the value of $c$ is 1, and is thus independent from any other possible random variable. We can rewrite $Z$ as $Z = E[CX_1X_2]$, and given that all three variables are independent, we have $Z = CE[X_1X_2]$ and since $C$ is always $c$ we finally have $Z = cE[X_1X_2]$ for all constants c.
	\\\\ We now proceed with the target claim.
	\begin{align}
	V(Y) &= E[Y^2] - (E[Y])^2 && \text{definition of V()} \\
	     &= E[(X_1+X_2)^2]-(E[X_1+X_2])^2 && \text{definition of Y} \\
		 &= E[{X_1}^2+2X_1X_2 + {X_2}^2] - (E[X_1]+E[X_2])^2 && \text{expand, $E[X+Y] = E[X] + E[Y]$} \\
	     &= E[{X_1}^2] + E[2X_1X_2] + E[{X_2}^2]- {E[X_1]}^2 - 2E[X_1]E[X_2]-{E[X_2]}^2 && E[X\cdot Y]=E[X]\cdot E[Y]\text{, expand} \\
	     &= E[{X_1}^2] + 2E[X_1]E[X_2] + E[{X_2}^2]- {E[X_1]}^2 - 2E[X_1]E[X_2]-{E[X_2]}^2 && \text{property shown above} \\
	     &= E[{X_1}^2] + E[{X_2}^2] - {E[X_1]}^2 - {E[X_2]}^2 && \text{algebra} \\
	     &= V(X_1) + V(X_2) && \text{definition of V()} 
	\end{align}
	Therefore we have that $V(Y) = V(X_1) + V(X_2)$, where $X_1,X_2$ are random variables and $Y = X_1 + X_2$.
	\item Prove the extension of Chebyshev's inequality for sums.
	\\\\ We proceed by direct proof from the defined variables and Chebyshev's inequality for single random variables. 
	\begin{align}
	Pr(|Y - \mu| \geq a) &\leq \frac{V(Y)}{a^2} && \text{definition of Chebyshev's} \\
	&\leq \frac{V(\sum_{i=1}^N \frac{1}{n}X_i)}{a^2} && \text{definition of Y} \\
	&\leq \frac{\sum_{i=1}^N V(\frac{1}{n}X_i)}{a^2} && \text{shown in 1} \\
	&\leq \frac{\sum_{i=1}^N V(X_i)}{N^2a^2} && \text{pull out constant, squared since $V = \sigma^2$} \\
	&\leq \frac{V(X_i)}{Na^2} = \frac{\sigma^2}{Na^2} && \text{simplify} 
	\end{align}
	Therefore, proceeding from the definitions given, the proven Chebyshev's inequality for random variables, and the property shown in 1, we have that $Pr(|Y - \mu| \geq a) \leq \frac{\sigma^2}{Na^2}$.
\end{enumerate}
	

\end{document}

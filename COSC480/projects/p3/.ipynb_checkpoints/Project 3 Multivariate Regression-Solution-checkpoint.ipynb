{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "This notebook will guide you through implementation of **multivariate linear regression** to to solve the **polynomial regression** problem:\n",
    "\n",
    "$$\n",
    "h_{w}(x) = w_0 + w_1 x + w_2 x^2 +  w_3 x^3 + w_4 x^4\n",
    "= \\boldsymbol{w}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} = \\begin{bmatrix}w_0 & w_1 & w_2 & w_3 & w_4\\end{bmatrix}^T, \n",
    "\\qquad\n",
    "\\mathbf{x} = \\begin{bmatrix}1 & x & x^2 & x^3 & x^4\\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "Below, you will\n",
    "\n",
    "1. Implement the squared error function for multivarate linear regression\n",
    "1. Implement gradient descent for multivariate linear regression\n",
    "1. Experiment with feature normalization to improve the convergence of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Review the helper functions in `mlr`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data set for polynomial regression\n",
    "\n",
    "Read ``generate_data`` in ``mlr`` and then call it to generate training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr import generate_dataset\n",
    "x, y = generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting data.   In general, your plots should be nice with labeled axes, title, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIP\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(x, y)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear transformation\n",
    "\n",
    "This data is clearly non-linear.  Simple linear regression will yield a poor fit to this data.\n",
    "\n",
    "We will use a non-linear feature transformation, $x \\mapsto [1, x, x^2, x^3, x^4]$, to feed into multivariate regression.  By fitting a \"line\" through these features, we are effectively fitting a polynomial of degree 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr import feature_expansion\n",
    "d = 4\n",
    "X = feature_expansion(x, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code reflection\n",
    "\n",
    "The function ``feature_expansion`` accepts an vector of $n$ scalar x values and returns an $n \\times 5$ data matrix by applying the feature expansion $x \\mapsto [1, x, x^2, x^3, x^4]$ to each scalar $x$ value.  \n",
    "\n",
    "In the space below, explain what is happening in each line of ``feature_expansion``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *TODO: write your answer here* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the error function\n",
    "Complete ``error_function`` which should compute the squared error for multivariate linear regression.  \n",
    "\n",
    "$$ E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^N \\left(f(x^{(i)}) - h_{\\mathbf{w}}(x^{(i)}) \\right)^2 $$\n",
    "\n",
    "Your implement should be *general* and work for any input data and weights (and not specific to this degree 4 polynomial stuff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the error function\n",
    "Run this to test your error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr import error_function\n",
    "\n",
    "# generate a few different weight vectors\n",
    "np.random.seed(1)\n",
    "w_random = np.random.rand(d+1)\n",
    "w_zeros  = np.zeros(d+1)\n",
    "w_ones   = np.ones(d+1)\n",
    "\n",
    "print(\"Error function (random): %.2f\" % error_function(X, y, w_random))  # prints 1090.47\n",
    "print(\"Error function  (zeros): %.2f\" % error_function(X, y, w_zeros))   # prints 16.91\n",
    "print(\"Error function   (ones): %.2f\" % error_function(X, y, w_ones))    # prints 50493.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First training algorithm: normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``normal_equations`` has been provided for you.  It returns the best-fit weight vector using the closed form solution.  This may be helpful in debugging your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlr import normal_equations, plot_model\n",
    "\n",
    "w_best = normal_equations(X, y)\n",
    "\n",
    "plot_model(x, y, w_best)\n",
    "print (\"Error function: %.2f\" % error_function(X, y, w_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement second training algorithm: (vectorized) gradient descent\n",
    "\n",
    "Implement gradient descent for multivariate linear regression. Make sure your solution is vectorized.  Your implementation should go in ``gradient_descent`` in ``mlr``.  Your implementation should be general and work for any X, y training data (i.e., not limited to the specific degree 4 polynomial data in this project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gradient descent to train the model\n",
    "* Write code to call your ``gradient_descent`` method to learn weights\n",
    "* Plot the model fit (use ``plot_model``)\n",
    "* Plot the error function vs. iteration to help assess convergence\n",
    "* Print the final value of the error function\n",
    "* Experiment with different step sizes and numbers of iterations until you can find a good hypothesis. Try to match the error runction value from ``normal_equations`` to two significant digits. How many iterations does this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write code here, feel free to add helper methods to mlr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many iterations does it take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *TODO: write your answer here* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIP\n",
    "from mlr import gradient_descent, gradient_descent_slow\n",
    "\n",
    "x, y = generate_dataset()\n",
    "d = 4\n",
    "X = feature_expansion(x, d)\n",
    "iters = 300000\n",
    "eta = .00001\n",
    "w, Ein_history = gradient_descent(X, y, eta, iters)\n",
    "\n",
    "plot_model(x, y, w)\n",
    "plt.show()\n",
    "print (\"Error function for best w: %.2f\" % error_function(X, y, w_best))\n",
    "print (\"Error function for gd w: %.2f\" % error_function(X, y, w))\n",
    "\n",
    "# END STRIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END STRIP\n",
    "\n",
    "plt.plot(range(len(Ein_history)), Ein_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('$E_in$')\n",
    "plt.title('$E_in$ vs. iterations')\n",
    "# TODO: add log scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with feature normalization\n",
    "\n",
    "You should have observed that it takes many iterations of gradient descent to match the error function value achieved by the normal equations. Now you will implement feature normalization to improve the convergence of gradient descent. The formula for feature normalization is:\n",
    "\n",
    "$$x^{(i)}_j \\leftarrow \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "Here are some guidelines for the implementation:\n",
    "\n",
    "* The values $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of\n",
    "the $j$th column (i.e., feature) in the **training data**. (Hint:\n",
    "there are numpy functions to compute these.)\n",
    "\n",
    "* Do not normalize the first feature, $x_0$, which is the bias term and always equal to 1. (Optional question: why?)\n",
    "\n",
    "* Use broadcasting to do the normalization---don't write for loops\n",
    "\n",
    "* After normalizing the training data and running gradient descent to obtain a weight vector, you need to translate that weight vector back to the original unnormalized feature values.  See question below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-normalizing weights\n",
    "How do you \"unnormalize\" the weights?  Let's use $\\mathbf{\\hat{w}} = [\\hat{w}_0, \\hat{w}_1, \\dots, \\hat{w}_d]^t$ to denote the weights found on the *normalized* training data.  The key is unnormalizing is to make sure you make the *same* prediction on the unnormalized data as you make on the normalized data.\n",
    "\n",
    "Let $\\mathbf{x}$ be a training example and let $\\mathbf{\\hat{x}}$ be this same example after normalization.  Since $\\mathbf{\\hat{x}}$ is normalized, $\\hat{x}_j = \\frac{x_j - \\mu_j}{\\sigma_j}$ for each feature $j = 1 \\dots d$; note that $\\hat{x}_0$ is not normalized, so $\\hat{x}_0 = x_0 = 1$.\n",
    "\n",
    "Our prediction on a normalized example is:\n",
    "$h_{\\mathbf{\\hat{w}}}(\\mathbf{\\hat{x}}) = \\sum_{j=0}^d\\hat{w}_j \\hat{x}_j = \\hat{w}_0 x_0 + \\sum_{j=1}^d\\hat{w}_j \\frac{x_j - \\mu_j}{\\sigma_j}$.\n",
    "\n",
    "Our prediction on an unnormalized example is simply:\n",
    "$h_{\\mathbf{{w}}}(\\mathbf{{x}}) = \\sum_{j=0}^d {w}_j {x}_j$.  But the question is: how do we derive $\\mathbf{w}$ from $\\mathbf{\\hat{w}}$?\n",
    "\n",
    "\n",
    "Write an expression for the unnormalized weights.  The $w_j$ terms should be fairly simple except for $w_0$ which be a little messy.  Your solution for $\\mathbf{{w}}$ should satisfy: $h_{\\mathbf{{w}}}(\\mathbf{{x}}) = h_{\\mathbf{\\hat{w}}}(\\mathbf{\\hat{x}})$ for any $\\mathbf{x}$ and its corresponding normalized version $\\mathbf{\\hat{w}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *TODO: write your answer here* ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat experiment\n",
    "Follow the same steps as above to experiment with gradient descent using the *normalized* training: print the value of the error function, and create the same plots. Tune the step size and number of iterations again to make gradient descent converge as quickly as possible. How many iterations does it take to match the Error function value from ``normal_equations`` to two decimal places?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write code here, feel free to add helper methods to mlr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many iterations, after feature normalization, does it take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *TODO: write your answer here* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIP\n",
    "from mlr import gradient_descent, normalize\n",
    "\n",
    "x, y = generate_dataset()\n",
    "d = 4\n",
    "X = feature_expansion(x, d)\n",
    "X_norm, mu, sigma = normalize(X)\n",
    "iters = 5000\n",
    "eta = .1\n",
    "w_norm, Ein_history = gradient_descent(X_norm, y, eta, iters)\n",
    "\n",
    "sigma2 = np.concatenate([[1], sigma])\n",
    "w = w_norm / sigma2\n",
    "\n",
    "w_0 = w_norm[0] - sum(w_norm[i] * mu[i-1] / sigma2[i] for i in range(1, len(w_norm)))\n",
    "w[0] = w_0\n",
    "\n",
    "plot_model(x, y, w)\n",
    "plt.show()\n",
    "print (\"Error function for best w: %.2f\" % error_function(X, y, w_best))\n",
    "print (\"Error function for gd w: %.2f\" % error_function(X, y, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIP\n",
    "\n",
    "plt.plot(range(len(Ein_history)), Ein_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('$E_{in}$')\n",
    "plt.title('$E_{in}$ vs. iterations')\n",
    "# TODO: add log scale?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
